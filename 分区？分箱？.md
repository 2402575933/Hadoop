首先说明一下分区和分箱的区别：<br>

**分区：** ：
<br> 分区指定多个Reduce时，默认的分区partition策略是利用hashPartitioner进行分区，但是这种对于输入数据Key的随机性而言容易发生数据倾斜的情况，导致进程卡死在某一个地方。支持自定义的Partitioner，
继承Partitioner，通过重写getPartition方法进行自定义划分数据到具体的文件中，虽然各种数据都能通过自定义的模式分配到指定的位置，但是仅仅通过文件名不能有什么代表性，代表哪块分区情况。每一个分区派给指定的Reduce进行执行。

**分箱：**<br>
分箱就是多路径输出，上面分区之后指定内容被发送到指定reduce，一个reduce产生一个part-r-xxxx的文件，尽管处理后的内容已经分文件输出，但是根据文件名字并不能直观的找到相应内容，尤其是文件相对较多时。<br>
为什么使用分箱而不是用getPartition，因为getPartition需要在执行任务之前明确有几个分区字段，并且需要自己主动指定每个分区，以便让每个分区都对应到具体的reducer任务中，形成不同的文件输出，可能涉及到的问题是多个reducer处理小量的数据，但是最佳的状态是使用更少的reducer做更多的事情，并且自定义指定分区会因为数据本身的某分区字段的缺失而使某个reduce程序废弃，浪费资源，显然不如程序自己分配分区的任务高效。
使用MultipleOutputs，每个实际的reduce任务能写出多个文件，并且支持文件名的自定义及其动态分配。



- 在reduce或者map类中创建MultipleOutputs对象（最好初始化在setup方法中），在cleanup中关闭流。<br>
```java
class TestReducer extends Reducer<Text, Text, Text, Text>{  
  
    //将结果输出到多个文件或多个文件夹
    private MultipleOutputs mos;  
 
    protected void setup(Context context) throws IOException,InterruptedException {  
        mos = new MultipleOutputs<>(context);  // 初始化mos
     }  
          
      
    protected void cleanup(Context context) throws IOException,InterruptedException {  
        mos.close();  //关闭对象  
    }  
} 
```
- 在map或reduce方法中使用MultipleOutputs对象输出数据，代替context.write();
```java
protected void reduce(Text key, Iterable<Text> values, Context context)  
            throws IOException, InterruptedException {  
        .... // 计算key和value
        //使用MultipleOutputs对象输出数据  
        if(key.toString().equals("file1")){  
            mos.write("file1", key, value, fileName);  
        }else if(key.toString().equals("file2")){  
            mos.write("file2", key, value, fileName);    
        }
} 
```

在main方法中添加如下代码：

```java
MultipleOutputs.addNamedOutput(job, namedOutput, TextOutputFormat.class, Text.class, NullWritable.class);
```

**代码实例：**
[CLickHere](https://github.com/2402575933/Hadoop/tree/main/example/multiple)
