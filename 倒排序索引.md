实现倒排序索引需要的是转换的思想，既如何灵活的转换数据的格式才能在最后形成目标格式，如何设计层，需要什么层等。<br>
最终的输出格式是：
单词+对应的文件中出现的次数。

```java
word : file1.txt-->1, file2.txt-->2 .....
mapreduce : file1.txt-->1 ...
```

**设计思路：** <br>
在map端获取到文件名，将单词和文件名拼接作为outKey，将1作为outValue，在Combiner中统计每个单词在相同文件中的出现次数，然后单词作为outKey，fileName + 出现次数作为outValue，向Reduce端输入，
Reduce端接收到输入后进行字符串拼接，输出即可。

**代码如下：**

```java
// Mapper
package com.tommy.exercise.backIndex;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

public class BackIndexMapper extends Mapper<LongWritable, Text, Text, Text> {

    private String fileName;

    private Text outK = new Text();
    private Text outV = new Text();

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        FileSplit fileSplit = (FileSplit) context.getInputSplit();
        fileName = fileSplit.getPath().getName();
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] split = line.split("\\s");
        outV.set("1");
        for (String word : split) {
            String initK = fileName + "\t" + word;
            outK.set(initK);
            context.write(outK, outV);
        }
    }
}

```

```java
// reducer
package com.tommy.exercise.backIndex;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class BackIndexReducer extends Reducer<Text, Text, Text, Text> {

    private Text outV = new Text();

    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        String initoutV = "";
        for (Text value : values) {
            initoutV = initoutV + "--" + value;
        }
        outV.set(initoutV);
        context.write(key, outV);
    }
}
```

```java
// Driver
package com.tommy.exercise.backIndex;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class BackIndexDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(BackIndexDriver.class);

        job.setMapperClass(BackIndexMapper.class);
        job.setReducerClass(BackIndexReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        job.setCombinerClass(BackIndexCombiner.class);

        FileInputFormat.setInputPaths(job, new Path("C:\\Users\\TOMMY\\Desktop\\backIndex"));
        FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\TOMMY\\Desktop\\backIndexOutPut"));

        boolean result = job.waitForCompletion(true);

        System.exit(result ? 0 : 1);
    }
}
```

```java
// Combiner
package com.tommy.exercise.backIndex;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class BackIndexCombiner extends Reducer<Text, Text, Text, Text> {
    private Text outK = new Text();
    private Text outV = new Text();

    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

        int sum = 0;

        for (Text value : values) {
            sum += Integer.parseInt(value.toString());
        }
        String line = key.toString();
        String[] split = line.split("\t");
        String fileName = split[0];
        String word = split[1];
        String initK = word;
        String initV = fileName + "->" + sum;
        outK.set(initK);
        outV.set(initV);
        context.write(outK, outV);
    }
}
```
